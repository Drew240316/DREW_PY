{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6ì¥. í…ìŠ¤íŠ¸ ë‹¤ë£¨ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ë…¸íŠ¸ë¶ì„ ì£¼í”¼í„° ë…¸íŠ¸ë¶ ë·°ì–´(nbviewer.jupyter.org)ë¡œ ë³´ê±°ë‚˜ êµ¬ê¸€ ì½”ë©(colab.research.google.com)ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://nbviewer.org/github/rickiepark/machine-learning-with-python-cookbook/blob/master/06.ipynb\"><img src=\"https://jupyter.org/assets/share.png\" width=\"60\" />ì£¼í”¼í„° ë…¸íŠ¸ë¶ ë·°ì–´ë¡œ ë³´ê¸°</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/machine-learning-with-python-cookbook/blob/master/06.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />êµ¬ê¸€ ì½”ë©(Colab)ì—ì„œ ì‹¤í–‰í•˜ê¸°</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 í…ìŠ¤íŠ¸ ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_data = [\"   Interrobang. By Aishwarya Henriette     \",\n",
    "             \"Parking And Going. By Karl Gautier\",\n",
    "             \"    Today Is The night. By Jarek Prakash   \"]\n",
    "\n",
    "# ê³µë°± ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë§ˆì¹¨í‘œë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "def capitalizer(string: str) -> str: # ë§¤ê°œë³€ìˆ˜ sê°€ ë¬¸ìì—´(str)ì´ì–´ì•¼ í•œë‹¤ëŠ” íƒ€ì… íŒíŠ¸ #ì´ í•¨ìˆ˜ê°€ ë¬¸ìì—´ì„ ë°˜í™˜í•œë‹¤ëŠ” íƒ€ì… íŒíŠ¸\n",
    "    return string.upper()\n",
    "\n",
    "# í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T03:07:39.947856Z",
     "start_time": "2025-06-05T03:07:39.810207Z"
    }
   },
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "import re\n",
    "\n",
    "# í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "\n",
    "#re.sub(ì¹˜í™˜í•  ì •ê·œì‹ {ë¬¸ìì—´ ë˜ëŠ” ì»´íŒŒì¼ëœ ê°ì²´}, ì¹˜í™˜í•  ë¬¸ìì—´, ê²€ìƒ‰/ì¹˜í™˜í•  ì›ë³¸ ë¬¸ìì—´\n",
    "# r\"\"ëŠ” raw string(ì›ì‹œ ë¬¸ìì—´)ì´ê³ , ì •ê·œì‹ì„ ì“¸ ë•Œ ì‚¬ìš©\n",
    "# r\"...\"ì€ ë¬¸ìì—´ ì•ˆì˜ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì(ì˜ˆ: \\n, \\t)ë¥¼ ê·¸ëŒ€ë¡œ ì²˜ë¦¬\n",
    "# í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_periods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_50104\\443035091.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# r\"...\"ì€ ë¬¸ìì—´ ì•ˆì˜ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì(ì˜ˆ: \\n, \\t)ë¥¼ ê·¸ëŒ€ë¡œ ì²˜ë¦¬\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;31m# í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[1;33m[\u001B[0m\u001B[0mreplace_letters_with_X\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstring\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mstring\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mremove_periods\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'remove_periods' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 HTML íŒŒì‹±ê³¼ ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n       Masego Azra'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ì˜ˆì œ HTML ì½”ë“œë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "html = \"\"\"\n",
    "       <div class='full_name'><span style='font-weight:bold'>\n",
    "       Masego</span> Azra</div>\"\n",
    "       \"\"\"\n",
    "\n",
    "# htmlì„ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# \"full_name\" ì´ë¦„ì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ì•„ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "soup.find(\"div\", { \"class\" : \"full_name\" }).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 êµ¬ë‘£ì  ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "import unicodedata  # ì „ ì„¸ê³„ ëª¨ë“  ë¬¸ì(ê¸°í˜¸, ì´ëª¨ì§€ í¬í•¨)ë¥¼ í•˜ë‚˜ì˜ í‘œì¤€ ì½”ë“œë¡œ í‘œí˜„ # ASCII ë“±.\n",
    "import sys  # ì…ì¶œë ¥, ì¸ì, ì¢…ë£Œ ë“± ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥ ì œê³µ\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "             '10000% Agree!!!! #LoveIT',\n",
    "             'Right?!?!']\n",
    "\n",
    "# êµ¬ë‘ì  ë¬¸ìë¡œ ì´ë£¨ì–´ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                            #íŒŒì´ì¬ì´ ì§€ì›í•˜ëŠ” ìœ ë‹ˆì½”ë“œ ë¬¸ì ì¤‘ ê°€ì¥ í° ì½”ë“œ í¬ì¸íŠ¸ ë²ˆí˜¸\n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# ë¬¸ìì—´ì˜ êµ¬ë‘ì ì„ ì‚­ì œí•©ë‹ˆë‹¤.\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ë¬¸ì   \tìœ ë‹ˆì½”ë“œ ì½”ë“œí¬ì¸íŠ¸\t10ì§„ìˆ˜\tí•¨ìˆ˜\n",
    "'A'  \tU+0041\t65\t        chr(65) â†’ 'A'\n",
    "'ê°€'\tU+AC00\t44032\t    chr(44032) â†’ 'ê°€'\n",
    "\n",
    "\n",
    "1.dict.fromkeys()ë¥¼ í†µí•´ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "êµ¬ë¬¸:\n",
    "\n",
    "dict.fromkeys(ì´í„°ëŸ¬ë¸”)\n",
    "ì˜ë¯¸: ì´í„°ëŸ¬ë¸”ì˜ ìš”ì†Œë“¤ì„ keyë¡œ, ê°’ì€ ëª¨ë‘ ê¸°ë³¸ê°’ Noneìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "\n",
    "python\n",
    "\n",
    "dict.fromkeys([1, 2, 3])\n",
    "ğŸ‘‰ {1: None, 2: None, 3: None}\n",
    "\n",
    "2. i in range(sys.maxunicode)ë¡œ ìœ ë‹ˆì½”ë“œ ì „ì²´ ë²”ìœ„ë¥¼ ìˆœíšŒ\n",
    "sys.maxunicodeëŠ” 1114111 (0x10FFFF) â†’ ìœ ë‹ˆì½”ë“œì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì½”ë“œí¬ì¸íŠ¸\n",
    "\n",
    "iëŠ” ì •ìˆ˜ (ì˜ˆ: 33, 46 ë“±)\n",
    "\n",
    "3. chr(i)ëŠ” ì •ìˆ˜ â†’ ë¬¸ì ë³€í™˜ í•¨ìˆ˜\n",
    "ğŸ’¬ ë„ˆ ì§ˆë¬¸: char()ì€ ë¬¸ìë¥¼ ì •ìˆ˜ë¡œ ë°”ê¾¸ë‚˜? â†’ âŒ ì•„ë‹ˆì•¼\n",
    "â†’ ì •ë‹µ: chr()ëŠ” ì •ìˆ˜ â†’ ë¬¸ì\n",
    "â†’ ë°˜ëŒ€ëŠ” ord() : ë¬¸ì â†’ ì •ìˆ˜\n",
    "\n",
    "í•¨ìˆ˜\tì„¤ëª…\tì˜ˆì‹œ\n",
    "chr(65)\t65ë¥¼ ë¬¸ìë¡œ â†’ 'A'\n",
    "ord('A')\t'A'ë¥¼ ì •ìˆ˜ë¡œ â†’ 65\n",
    "\n",
    "4. unicodedata.category(chr(i))ë¡œ ë¬¸ì ë¶„ë¥˜ ì¡°íšŒ\n",
    "ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ë²”ì£¼(category)ë¡œ ë¶„ë¥˜í•¨\n",
    "\n",
    "ì˜ˆ:\n",
    "\n",
    "'A' â†’ 'Lu' (ëŒ€ë¬¸ì)\n",
    "\n",
    "'!' â†’ 'Po' (ê¸°íƒ€ êµ¬ë‘ì )\n",
    "\n",
    "',' â†’ 'Po'\n",
    "\n",
    "â†’ ì¦‰, chr(i)ê°€ êµ¬ë‘ì ì´ë©´ 'P'ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ìì—´ì„ ë°˜í™˜\n",
    "\n",
    "5. .startswith('P')ëŠ” ê·¸ ì¹´í…Œê³ ë¦¬ê°€ êµ¬ë‘ì  ê³„ì—´ì¸ì§€ í™•ì¸\n",
    "'P'ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë§Œ ê³¨ë¼ëƒ„:\n",
    "\n",
    "Po (ê¸°íƒ€)\n",
    "\n",
    "Pd (ëŒ€ì‹œ)\n",
    "\n",
    "Ps (ì‹œì‘ ê´„í˜¸)\n",
    "\n",
    "ë“±ë“±\n",
    "\n",
    "\n",
    "ìë£Œí˜• : ë¬¸ì to ë”•ì…”ë„ˆë¦¬ ( í‚¤ : ìˆ«ì , ê°’ : none )\n",
    "ì •ì˜ : êµ¬ë‘ì ì„ ì „ë¶€ replaceí•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì—, ìœ ë‹ˆì½”ë“œ í‘œí˜„ pë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì„ ì°¾ì•„ ì‚­í•˜ì\n",
    "ê³ ë ¤ : ìœ ë‹ˆì½”ë“œëŠ” p lê³¼ ê°™ì€ êµ¬ë‘ì , ëŒ€ë¬¸ì ê³„ì—´ì´ ìˆê³ , ê°ê° ë²ˆí˜¸ê°€ ë¶€ì—¬ë¨. , ìµœëŒ€ ìœ ë‹ˆì½”ë“œëŠ” maxunicode\n",
    "í•„ìš”í•œ ê°ì²´ : ì…ë ¥ê°’, êµ¬ë‘ì ì„ ì°¾ëŠ” ê°ì²´\n",
    "í•„ìš”í•œ í•¨ìˆ˜ : sys (ìµœëŒ€ ìœ ë‹ˆì½”ë“œ ì •ìˆ˜) , unicode ì „ì„¸ê³„ëª¨ë“  ìœ ë‹ˆì½”ë“œ ( ë¬¸ì 'A' , ìœ ë‹ˆì½”ë“œ U_001, ì½”ë“œ í¬ì¸íŠ¸ 65, 10ì§„ìˆ˜? char(ì •ìˆ˜ ìš”ì†Œ ê°’ë“¤) -> 65ë¡œ ë³€ê²½),  category(ìœ ë‹ˆì½”ë“œ ë²”ì£¼) , ì‚­ì œ? translate(ê°ì²´)\n",
    "ì¡°ê±´ë¬¸ & ë¡œì§\n",
    "    1. ì„í¬íŠ¸í•œë‹¤.\n",
    "    2. ëª¨ë“  êµ¬ë‘ì ë“¤ì„ ì°¾ê¸° ìœ„í•´, ìµœëŒ€ ìœ ë‹ˆì½”ë“œ ì •ìˆ˜ë¥¼ ìˆœíšŒí•´ì„œ ì½”ë“œ í¬ì¸íŠ¸ë¥¼ ì°¾ê³ , ì´ ì •ìˆ˜ë¥¼ ë¬¸ìë¡œ ë³€ê²½í•´ì„œ ë¬¸ìê°€ p ê³„ì—´ì˜ ë²”ì£¼ì¸ì§€ ì°¾ëŠ”ë‹¤.\n",
    "    3. ì‚­ì œí•œë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 í…ìŠ¤íŠ¸ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/haesun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# êµ¬ë‘ì  ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# ë‹¨ì–´ë¥¼ í† í°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "# ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 ë¶ˆìš©ì–´ ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/haesun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¶ˆìš©ì–´ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ë‹¨ì–´ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "tokenized_words = ['i',\n",
    "                   'am',\n",
    "                   'going',\n",
    "                   'to',\n",
    "                   'go',\n",
    "                   'to',\n",
    "                   'the',\n",
    "                   'store',\n",
    "                   'and',\n",
    "                   'park']\n",
    "\n",
    "# ë¶ˆìš©ì–´ë¥¼ ì ì¬í•©ë‹ˆë‹¤.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# ë¶ˆìš©ì–´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¶ˆìš©ì–´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CorpusReader.abspath of <WordListCorpusReader in '/home/haesun/nltk_data/corpora/stopwords'>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.abspath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¶™ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 179)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "len(ENGLISH_STOP_WORDS), len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serious', 'side', 'thru', 'hers', 'keep']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ENGLISH_STOP_WORDS)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 ì–´ê°„ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# ë‹¨ì–´ í† í°ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "\n",
    "# ì–´ê°„ ì¶”ì¶œê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# ì–´ê°„ ì¶”ì¶œê¸°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 í’ˆì‚¬ íƒœê¹…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/haesun/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íƒœê±°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# ì‚¬ì „ í›ˆë ¨ëœ í’ˆì‚¬ íƒœê¹…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# í’ˆì‚¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ì–´ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "\n",
    "# ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "tagged_tweets = []\n",
    "\n",
    "# ê° ë‹¨ì–´ì™€ íŠ¸ìœ—ì„ íƒœê¹…í•©ë‹ˆë‹¤.\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "# ì›-í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ íƒœê·¸ë¥¼ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŠ¹ì„± ì´ë¦„ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/haesun/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¸Œë¼ìš´ ì½”í¼ìŠ¤ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "# ë¸Œë¼ìš´ ì½”í¼ìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ë‹¤ìŒ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# 4,000ê°œì˜ ë¬¸ì¥ì€ í›ˆë ¨ìš©ìœ¼ë¡œ 623ê°œëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# ë°±ì˜¤í”„ íƒœê·¸ ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# ì •í™•ë„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¶™ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ë‹¤ìŒ ì£¼ì„ì„ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "#!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì‹  ë²„ì „ì˜ tweepy íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë  ê²½ìš° konlpyì—ì„œ StreamListenerê°€ ì—†ë‹¤ëŠ” ì—ëŸ¬ê°€ ë°œìƒí•˜ë¯€ë¡œ \n",
    "# 3.10ë²„ì „ì„ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”. pip install tweepy==3.10\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('íƒœì–‘ê³„', 'Noun'),\n",
       " ('ëŠ”', 'Josa'),\n",
       " ('ì§€ê¸ˆ', 'Noun'),\n",
       " ('ìœ¼ë¡œë¶€í„°', 'Josa'),\n",
       " ('ì•½', 'Noun'),\n",
       " ('46ì–µ', 'Number'),\n",
       " ('ë…„', 'Noun'),\n",
       " ('ì „', 'Noun'),\n",
       " (',', 'Punctuation'),\n",
       " ('ê±°ëŒ€í•œ', 'Adjective'),\n",
       " ('ë¶„ì', 'Noun'),\n",
       " ('êµ¬ë¦„', 'Noun'),\n",
       " ('ì˜', 'Josa'),\n",
       " ('ì¼ë¶€ë¶„', 'Noun'),\n",
       " ('ì´', 'Josa'),\n",
       " ('ì¤‘ë ¥', 'Noun'),\n",
       " ('ë¶•ê´´', 'Noun'),\n",
       " ('ë¥¼', 'Josa'),\n",
       " ('ì¼ìœ¼í‚¤ë©´ì„œ', 'Verb'),\n",
       " ('í˜•ì„±', 'Noun'),\n",
       " ('ë˜ì—ˆë‹¤', 'Verb')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'íƒœì–‘ê³„ëŠ” ì§€ê¸ˆìœ¼ë¡œë¶€í„° ì•½ 46ì–µ ë…„ ì „, ê±°ëŒ€í•œ ë¶„ì êµ¬ë¦„ì˜ ì¼ë¶€ë¶„ì´ ì¤‘ë ¥ ë¶•ê´´ë¥¼ ì¼ìœ¼í‚¤ë©´ì„œ í˜•ì„±ë˜ì—ˆë‹¤'\n",
    "\n",
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['íƒœì–‘ê³„',\n",
       " 'ëŠ”',\n",
       " 'ì§€ê¸ˆ',\n",
       " 'ìœ¼ë¡œë¶€í„°',\n",
       " 'ì•½',\n",
       " '46ì–µ',\n",
       " 'ë…„',\n",
       " 'ì „',\n",
       " ',',\n",
       " 'ê±°ëŒ€í•œ',\n",
       " 'ë¶„ì',\n",
       " 'êµ¬ë¦„',\n",
       " 'ì˜',\n",
       " 'ì¼ë¶€ë¶„',\n",
       " 'ì´',\n",
       " 'ì¤‘ë ¥',\n",
       " 'ë¶•ê´´',\n",
       " 'ë¥¼',\n",
       " 'ì¼ìœ¼í‚¤ë©´ì„œ',\n",
       " 'í˜•ì„±',\n",
       " 'ë˜ì—ˆë‹¤']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['íƒœì–‘ê³„', 'ì§€ê¸ˆ', 'ì•½', 'ë…„', 'ì „', 'ë¶„ì', 'êµ¬ë¦„', 'ì¼ë¶€ë¶„', 'ì¤‘ë ¥', 'ë¶•ê´´', 'í˜•ì„±']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 í…ìŠ¤íŠ¸ë¥¼ BoWë¡œ ì¸ì½”ë”©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# BoW íŠ¹ì„± í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# íŠ¹ì„± í–‰ë ¬ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŠ¹ì„± ì´ë¦„ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "# ì‚¬ì´í‚·ëŸ° 0.24 ë²„ì „ ì´í•˜ì¼ ê²½ìš° get_features_names() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì˜µì…˜ì„ ì§€ì •í•˜ì—¬ íŠ¹ì„± í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "                              stop_words=\"english\",\n",
    "                              vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "\n",
    "# íŠ¹ì„± í–‰ë ¬ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-ê·¸ë¨ê³¼ 2-ê·¸ë¨ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 ë‹¨ì–´ ì¤‘ìš”ë„ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Sweden is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# tf-idf íŠ¹ì„± í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# tf-idf íŠ¹ì„± í–‰ë ¬ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf íŠ¹ì„± í–‰ë ¬ì„ ë°€ì§‘ ë°°ì—´ë¡œ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŠ¹ì„± ì´ë¦„ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "tfidf.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
